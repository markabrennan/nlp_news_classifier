{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import config\n",
    "import logging\n",
    "import importlib\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import validators\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from calendar import monthrange\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import treebank\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "\n",
    "pd.options.display.max_columns = 300\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 999\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn import tree \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC  \n",
    "from time import time\n",
    "np.random.seed(0)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.sklearn_api import lsimodel, ldamodel\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, extra_stop_words, bigram_list, language='english', source=None):\n",
    "        self.source = source\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        self.more_stops = extra_stop_words\n",
    "        self.stopwords.update(self.more_stops) \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        self.bigram_list = bigram_list        \n",
    "        self.first_grams = [word.split()[0] for word in bigram_list]\n",
    "        self.second_grams = [word.split()[1] for word in bigram_list]\n",
    "\n",
    "    def get_custom_bigrams(self, tokens):\n",
    "        new_toks = []\n",
    "        length = len(tokens)\n",
    "\n",
    "        for ix, tok in enumerate(tokens):\n",
    "            try:\n",
    "                if tok in self.first_grams:\n",
    "                    if ix+1 <= length and tokens[ix+1] in self.second_grams:\n",
    "                        new_toks.append(' '.join([tok, tokens[ix+1]]))\n",
    "                        continue\n",
    "                if tok in self.second_grams:\n",
    "                    # we assume if we see the 2nd token in a bigram that\n",
    "                    # we've already prcessed the first and second together\n",
    "                    # so skip \n",
    "                    continue\n",
    "                new_toks.append(tok)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "        return new_toks        \n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(unicodedata.category(char).startswith('P') for char in token)\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "    \n",
    "    def get_stopwords(self):\n",
    "        return self.stopwords\n",
    "\n",
    "    def get_wordnet_pos(selfm, word):\n",
    "        #Map POS tag to first character lemmatize() \n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def normalize(self, document):\n",
    "        NYT_PAT = '^[A-Z]* — '\n",
    "        NYT_COMP_PAT = re.compile(NYT_PAT)        \n",
    "        \n",
    "        norm_toks = []\n",
    "    \n",
    "        if self.source and self.source == 'NYT':\n",
    "            document = NYT_COMP_PAT.sub('', document)\n",
    "        \n",
    "        if self.source and self.source == 'RT':\n",
    "            document = self.rt_clean(document)\n",
    "            \n",
    "        # NOTE:  I found nltk's sentenizer to perform poorly\n",
    "            #        for sent in sent_tokenize(document):\n",
    "        for sent in document.split('.'):            \n",
    "            for tok in nltk.word_tokenize(sent):\n",
    "                if self.is_punct(tok) or tok.isdigit():\n",
    "                    continue\n",
    "                lem = self.lemmatize(tok, self.get_wordnet_pos(tok)).lower()\n",
    "                if not self.is_stopword(lem):\n",
    "                    norm_toks.append(lem) \n",
    "#         bigram_toks = self.get_custom_bigrams(norm_toks)\n",
    "#         return ' '.join(bigram_toks)\n",
    "#         return ' '.join(norm_toks)       \n",
    "        return ' '.join(self.get_custom_bigrams(norm_toks))\n",
    "    \n",
    "    def rt_clean(self, text):\n",
    "        #\n",
    "        # first match this simple, typical Reuters pattern:\n",
    "        # \"(Reuters) - British Prime Minister Boris Johnson...\"\n",
    "        rt_pat = r'^\\(Reuters\\) - '\n",
    "        RT_PAT = re.compile(rt_pat)\n",
    "        text = RT_PAT.sub('', text)\n",
    "\n",
    "        rt_pat2 = r'^\\(This.*\\)'\n",
    "\n",
    "        RT_PAT2 = re.compile(rt_pat2)\n",
    "        text = RT_PAT2.sub('', text)\n",
    "\n",
    "        # match this:  \"ANKARA/Turkish-backed Syrian...\"\"\n",
    "        # or this:  \"MAMALLAPURAM, India, Chinese President...\"\"\n",
    "        # or this:  \"ADDIS For Samson Berhane, the news that...\"\"\n",
    "        caps_loc_pat = r'[A-Z]{2,20}[ \\/,]{1}'\n",
    "        CAPS_PAT = re.compile(caps_loc_pat)\n",
    "\n",
    "        # After removing first pattern, \n",
    "        # match this:  \"IndiaAt least five people were injured in a grenade attack\"\n",
    "        # but save \"At least...\"\n",
    "        name_against_text_pat = r'^ *[A-Z][a-z]{2,20}([A-Z]{1})'\n",
    "        NM_TEXT_PAT = re.compile(name_against_text_pat)\n",
    "\n",
    "        first_sub = CAPS_PAT.sub('', text)\n",
    "\n",
    "        match = NM_TEXT_PAT.match(first_sub)\n",
    "        rep_char = ''\n",
    "        if match:\n",
    "            rep_char = match[1]\n",
    "\n",
    "        t = NM_TEXT_PAT.sub('', first_sub)\n",
    "        # now add back that lone capital letter\n",
    "        text = rep_char + t\n",
    "\n",
    "        return text    \n",
    "\n",
    "\n",
    "    def lemmatize(self, token, pos):\n",
    "        return self.lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.source = y\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for doc in documents:\n",
    "            yield self.normalize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to pull out our custom bi-grams\n",
    "bigrams = ['donald trump', 'angela merkel', 'boris johnson', 'vladimir putin', 'benjamin netanyahu', \n",
    "           'hong kong', 'north korea', 'south korea', 'united kingdom', 'united states', 'south africa', \n",
    "           'xi jinping', 'carrie lam', 'oleksiy honcharuk', 'volodymyr zelensky', 'emmanuel macron',\n",
    "           'viktor orbán', 'justin trudeau', 'north america', 'south america', 'sinn fein', 'jeremy corbyn', 'narendra modi',\n",
    "            'mohamed morsi', 'shuping wang', 'hassan rouhani', 'rudy giuliani', 'joe biden', 'new zealand', 'european union', 'pope francis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stops = {'mr', 'ms', 'mr.', 'ms.', 'mrs', 'mrs.','say', 'said', 'saying', \n",
    "                           'also', 'yeh', 'hom', 'even', 'like', 'k', 'n', 'u', 'would', 'could', '$'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for pipeline automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fetch article (denote source)\n",
    "2. Scrape article\n",
    "3. Run transformation pipeline\n",
    "   - Use instantiated text normalizer \n",
    "   - Use instantiated bigram transformer\n",
    "   - \"stringify\" tokens (' '.join())\n",
    "   - Create PD Series of text data\n",
    "   - Vectorize text data, based on fitted TD-IDF vector model\n",
    "   - Pass vector to model.predict()\n",
    "   - Pull out predicted label\n",
    "   \n",
    " Questions:  should I use a SKLearn Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_run_pipeline(doc, model, vector, extra_stops=more_stops, bigrams=bigrams, source=None):\n",
    "    textNormer = TextNormalizer(extra_stop_words=more_stops, bigram_list=bigrams, source=source)\n",
    "    text = textNormer.normalize(doc)\n",
    "    print(text)\n",
    "    text_series = pd.Series(text)\n",
    "    print(text_series)\n",
    "    text_vec = vector.transform(text_series)\n",
    "    train_pred = model.predict(text_vec.toarray())\n",
    "    return train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "textNormer = TextNormalizer(extra_stop_words=more_stops, bigram_list=bigrams, source=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'russian cyber espionage unit hack iranian hacker lead attack country joint uk us investigation reveal so-called turla group link russian intelligence allegedly hijack tool oilrig group widely link iranian government accord two-year probe uk national cyber security centre collaboration us national security agency ncsc part gchq digital intelligence agency iranian group likely unaware hack method hack deployed another cyber espionage team security official involve investigation victims include military establishment government department scientific organisation university across world mainly middle east paul chichester ncsc director operation turla activity represent real change modus operandi cyber actor add sense confusion state-backed cyber group responsible successful attack reason publicise different tradecraft see turla use told reporter want others able understand activity chichester described turla begin piggyback oilrig attack monitoring iranian hack closely enough use backdoor route organisation gain access result intelligence turla know waterbug venomous bear russian group progress initiate attack use oilrig command-and-control infrastructure software organisations approximately country successfully hack way turla benefit operation oilrig collect operational output allow gain rapid access victim otherwise chichester make life much easy opportunistic operation give turla wealth information access otherwise kremlin respond request comment financial times russia government consistently deny behind hack attempt state president vladimir putin interview ft earlier year described allegation moscow orchestrate attempt influence us election mythical cyber espionage group increasingly conceal identity call false flag operation try mimic activity another group last year us intelligence agency report uncovered fact russian hacker attempt disrupt winter olympics pyeongchang south korea use line code associate lazarus group attribute north korea ncsc turla operation go far far imitation oilrig know name crambus apt34 hack never see level sophistication see chichester unique complexity scale sophistication actually really hard masquerade another entity turla potential hijack state-sponsored cyber group become crowd space see people innovate quite rapidly domain'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textNormer.normalize(test_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get our persisted model and TF-IDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = joblib.load('tfidf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('gnb-model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_article.pkl', 'rb') as f:\n",
    "    test_article = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Russian cyber espionage unit has hacked Iranian hackers to lead attacks in more than 35 countries, a joint UK and US investigation has revealed.\\n\\nThe so-called Turla group, which has been linked with Russian intelligence, allegedly hijacked the tools of Oilrig, a group widely linked to the Iranian government, according to a two-year probe by the UK’s National Cyber Security Centre in collaboration with the US’ National Security Agency. The NCSC is part of GCHQ, the digital intelligence agency.\\n\\nThe Iranian group is most likely unaware that its hacking methods have been hacked and deployed by another cyber espionage team, security officials involved in the investigation said. Victims include military establishments, government departments, scientific organisations and universities across the world, mainly in the Middle East.\\n\\nPaul Chichester, NCSC director of operations, said Turla’s activity represented “a real change in the modus operandi of cyber actors” which he said “added to the sense of confusion” over which state-backed cyber groups had been responsible for successful attacks.\\n\\n“The reason we are [publicising] this is because of the different tradecraft we are seeing Turla use,” he told reporters. “We want others to be able to understand this activity.”\\n\\nMr Chichester described how Turla began “piggybacking” on Oilrig’s attacks by monitoring an Iranian hack closely enough to use the same backdoor route into an organisation or to gain access to the resulting intelligence. Turla is also known as Waterbug or Venomous Bear.\\n\\nBut the Russian group then progressed to initiating their own attacks using Oilrig’s command-and-control infrastructure and software. Organisations in approximately 20 countries were successfully hacked in this way.\\n\\n“[Turla] could benefit from the operations of Oilrig. They could collect some of their operational output\\u2009.\\u2009.\\u2009.\\u2009It allowed them to gain more rapid access to victims than they would otherwise have done,” Mr Chichester said. “It made life much easier. This is an opportunistic operation which has given [Turla] a wealth of information and access they wouldn’t otherwise have had.”\\n\\nThe Kremlin did not respond to a request for comment from the Financial Times. Russia’s government has consistently denied it is behind hacking attempts on other states. President Vladimir Putin, in an interview with the FT earlier this year, described allegations that Moscow had orchestrated attempts to influence the 2016 US elections as “mythical”.\\n\\nCyber espionage groups are increasingly concealing their identities under so called “false flag” operations — in which they try to mimic the activities of another group. Last year US intelligence agencies were reported to have uncovered the fact that Russian hackers had attempted to disrupt the Winter Olympics in Pyeongchang, South Korea, using lines of code associated with Lazarus Group, attributed to North Korea.\\n\\nBut NCSC says Turla’s operations go far further than imitation, and that Oilrig itself — also known by the names Crambus and APT34 — was hacked.\\n\\n“We have never seen this done to the level of sophistication that we are seeing here,” Mr Chichester said. “It’s unique in the complexity and scale and sophistication. It’s actually really hard masquerading [as another entity].”\\n\\nHe said that Turla now had the potential to hijack other state-sponsored cyber groups. “This is becoming a very crowded space and we do see people innovate quite rapidly in that domain,” he said.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "russian cyber espionage unit hack iranian hacker lead attack country joint uk us investigation reveal so-called turla group link russian intelligence allegedly hijack tool oilrig group widely link iranian government accord two-year probe uk national cyber security centre collaboration us national security agency ncsc part gchq digital intelligence agency iranian group likely unaware hack method hack deployed another cyber espionage team security official involve investigation victims include military establishment government department scientific organisation university across world mainly middle east paul chichester ncsc director operation turla activity represent real change modus operandi cyber actor add sense confusion state-backed cyber group responsible successful attack reason publicise different tradecraft see turla use told reporter want others able understand activity chichester described turla begin piggyback oilrig attack monitoring iranian hack closely enough use backdoor route organisation gain access result intelligence turla know waterbug venomous bear russian group progress initiate attack use oilrig command-and-control infrastructure software organisations approximately country successfully hack way turla benefit operation oilrig collect operational output allow gain rapid access victim otherwise chichester make life much easy opportunistic operation give turla wealth information access otherwise kremlin respond request comment financial times russia government consistently deny behind hack attempt state president vladimir putin interview ft earlier year described allegation moscow orchestrate attempt influence us election mythical cyber espionage group increasingly conceal identity call false flag operation try mimic activity another group last year us intelligence agency report uncovered fact russian hacker attempt disrupt winter olympics pyeongchang south korea use line code associate lazarus group attribute north korea ncsc turla operation go far far imitation oilrig know name crambus apt34 hack never see level sophistication see chichester unique complexity scale sophistication actually really hard masquerade another entity turla potential hijack state-sponsored cyber group become crowd space see people innovate quite rapidly domain\n",
      "0    russian cyber espionage unit hack iranian hack...\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NYT'], dtype='<U3')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_and_run_pipeline(test_article, model, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# I think we're holding off on using SKLearn's pipeline, due to the necessity to know\n",
    "# the source of each article for our transformation - alas!\n",
    "#\n",
    "# def create_pipeline(estimator, stop_words, bigrams):\n",
    "#     steps = [\n",
    "#         ('normalize', TextNormalizer(stop_words, bigrams)),\n",
    "#         ('vectorize', TfidfVectorizer(stop_words='english', min_df=5, max_df=.5))\n",
    "#     ]\n",
    "#     steps.append(('classifier', estimator))\n",
    "#     return Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_doc_pop_df.pkl', 'rb') as f:\n",
    "    news_doc_pop_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>NYT</td>\n",
       "      <td>MOSCOW — Allies of President Vladimir V. Putin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>NYT</td>\n",
       "      <td>WASHINGTON — President Trump warned on Tuesday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106</th>\n",
       "      <td>RT</td>\n",
       "      <td>FRANKFURT/The German government on Friday pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>FT</td>\n",
       "      <td>Like many Mexican business people, Armando San...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>FT</td>\n",
       "      <td>Charles Li, chief executive of Hong Kong Excha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper                                               text\n",
       "240    NYT  MOSCOW — Allies of President Vladimir V. Putin...\n",
       "2306   NYT  WASHINGTON — President Trump warned on Tuesday...\n",
       "2106    RT  FRANKFURT/The German government on Friday pres...\n",
       "236     FT  Like many Mexican business people, Armando San...\n",
       "333     FT  Charles Li, chief executive of Hong Kong Excha..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_doc_pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6669"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_doc_pop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = news_doc_pop_df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = news_doc_pop_df[['paper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_pipeline(MultinomialNB(), more_stops, bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d768f88d541e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \"\"\"\n\u001b[1;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-14b35d26703b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-14b35d26703b>\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mnorm_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NYT'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNYT_COMP_PAT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1476\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m   1477\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
