{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import config\n",
    "import logging\n",
    "import importlib\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import validators\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from calendar import monthrange\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import treebank\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "\n",
    "pd.options.display.max_columns = 300\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 999\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn import tree \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC  \n",
    "from time import time\n",
    "np.random.seed(0)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.sklearn_api import lsimodel, ldamodel\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        self.more_stops = {'mr', 'ms', 'mr.', 'ms.', 'mrs', 'mrs.','say', 'also', 'yeh', 'hom'}\n",
    "        self.stopwords.update(self.more_stops) \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(unicodedata.category(char).startswith('P') for char in token)\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def get_wordnet_pos(selfm, word):\n",
    "        #Map POS tag to first character lemmatize() \n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def normalize(self, document, source):\n",
    "        NYT_PAT = '^[A-Z]* â€” '\n",
    "        NYT_COMP_PAT = re.compile(NYT_PAT)\n",
    "        norm_toks = []\n",
    "\n",
    "        if source == 'NYT':\n",
    "            document = NYT_COMP_PAT.sub('', document)\n",
    "\n",
    "        for sent in sent_tokenize(document):\n",
    "            for tok in nltk.word_tokenize(sent):\n",
    "                if self.is_stopword(tok) or self.is_punct(tok) or tok.isdigit():\n",
    "                    continue\n",
    "                norm_toks.append(self.lemmatize(tok, self.get_wordnet_pos(tok)).lower()) \n",
    "                \n",
    "        return norm_toks\n",
    "\n",
    "\n",
    "    def lemmatize(self, token, pos):\n",
    "        return self.lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for doc in documents:\n",
    "            yield self.normalize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txfm = partial(text_normalizer.normalize, source='NYT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_df['norm_text'] = nyt_df.text.apply(txfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = Dictionary([[word for word in doc] for doc in nyt_df.norm_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [lexicon.doc2bow(doc) for doc in nyt_df.norm_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [gmodel.model.named_steps['vect'].lexicon.doc2bow(doc),\n",
    "            for doc in gmodel.model.named_steps['norm'].transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel([lexicon.doc2bow(doc) for doc in nyt_df.norm_text], id2word=lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LdaModel(corpus, id2word=lexicon, num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.print_topics(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = lexicon.id2token\n",
    "\n",
    "for word_id, freq in next(iter(corpus)):\n",
    "    print(id2token[word_id], freq)\n",
    "    # get the highest weighted topic for each of the documents in the corpus\n",
    "\n",
    "def get_topics(vectorized_corpus, model):\n",
    "    from operator import itemgetter\n",
    "    topics = [\n",
    "                max(model[doc], key=itemgetter(1))[0]\n",
    "                for doc in vectorized_corpus]\n",
    "    return topics\n",
    "\n",
    "topics = get_topics(corpus, model)\n",
    "for topic, doc in zip(topics, nyt_df.norm_text):\n",
    "    print(\\\"Topic:{}\\\".format(topic))\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyLDAvis.gensim.prepare(model, corpus, lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gensim_data3.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(data, 'gensim3.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(model.num_topics):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud().fit_words(dict(model.show_topic(t, 200))))\n",
    "    plt.axis(\\\"off\\\")\n",
    "    plt.title(\\\"Topic #\\\" + str(t))\n",
    "    plt.savefig(f'nyt_word_cloud_{t}.png')\n",
    "    plt.show()\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
